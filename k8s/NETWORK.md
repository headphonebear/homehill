# Orchard â€“ Netzwerk-Konfiguration

Dieses Dokument beschreibt die Netzwerk-Architektur des Orchard-Clusters sowie die Integration in die bestehende Homehill-Infrastruktur.

---

## 1. Netzwerk-Ãœbersicht

Orchard nutzt das bestehende Homehill-Netzwerk und integriert sich nahtlos in die etablierte Infrastruktur:

- **Netzwerk-Segment**: Homehill LAN (`192.168.x.0/24` oder Ã¤hnlich)
- **DNS-Service**: Existierend (Pi-hole fÃ¼r lokale AuflÃ¶sung)
- **Externe Domain**: `homehill.de` (registriert, DNS bei Hetzner)
- **Geplante Wildcard**: `*.homehill.de` fÃ¼r Ingress-Services

---

## 2. Orchard-Nodes â€“ IP-Adressierung

### 2.1. Statische IPs oder DHCP-Reservation

Alle drei Orchard-Nodes sollten **zuverlÃ¤ssige, konsistente IP-Adressen** haben:

| Hostname | Rolle | IP-Adresse | MAC-Adresse | Anmerkung |
|----------|-------|------------|------------|-----------|
| `apple` | Control-Plane | `192.168.x.y` | `xx:xx:xx:xx:xx:xx` | Ãœber DHCP-Reservation oder statisch |
| `lemon` | Worker | `192.168.x.z` | `yy:yy:yy:yy:yy:yy` | Ãœber DHCP-Reservation oder statisch |
| `plum` | Worker | `192.168.x.w` | `zz:zz:zz:zz:zz:zz` | Ãœber DHCP-Reservation oder statisch |

**Empfehlung**: DHCP-Reservation ist wartungsfreundlicher als statische Konfiguration pro Node.  
(Konfiguration im Heimnetzwerk-Router oder DHCP-Server, z. B. Pi-hole.)

### 2.2. DNS-Namen fÃ¼r Nodes

Falls nicht bereits vorhanden, sollten DNS-EintrÃ¤ge fÃ¼r die Nodes im lokalen Netz existieren:

```text
apple.homehill.de    â†’ 192.168.x.y
lemon.homehill.de    â†’ 192.168.x.z
plum.homehill.de     â†’ 192.168.x.w
```

Diese werden Ã¼ber den lokalen DNS-Server (Pi-hole) aufgelÃ¶st.  
**Wichtig**: Das ermÃ¶glicht spÃ¤ter auch `K3S_URL="https://apple.homehill.de:6443"` statt nur IP-basiert.

---

## 3. Kubernetes API-Server Zugriff

### 3.1. Aktueller Zustand (Bootstrap)

WÃ¤hrend der ersten Installation wird die **IP-Adresse** des API-Servers verwendet:

```yaml
# ~/.kube/config
server: https://192.168.x.y:6443
```

Dies ist funktional, aber:
- Bei IP-Ã„nderungen (z. B. nach Neustart mit DHCP) bricht der Zugriff.
- FÃ¼r Production / GitOps ist ein stabiler FQDN mit TLS-Zertifikat besser.

### 3.2. Ziel: FQDN + TLS fÃ¼r Kubernetes API

Langfristig soll der API-Server Ã¼ber einen **stabilen FQDN** mit **gÃ¼ltigem TLS-Zertifikat** erreichbar sein.

Optionen:

**Option A: K3s selbst mit cert-manager**  
- K3s kann Ã¼ber die Installation mit `--tls-san=apple.homehill.de` konfiguriert werden.
- Dann braucht ein Zertifikat (via Let's Encrypt, selbstsigniert, etc.).

**Option B: Reverse Proxy vor K3s**  
- Ein Reverse Proxy (z. B. Traefik oder nginx) auÃŸerhalb des Clusters sitzt vor dem API-Server.
- Dieser Proxy terminiert TLS mit einem gÃ¼ltigen Zertifikat.

**Option C: Hetzner DNS + Traefik (spÃ¤ter, wenn GitOps lÃ¤uft)**  
- Nach Argo CD Setup: IngressRoute mit Let's Encrypt DNS-01 Challenge.
- Dies ist das langfristige, wartungsfreundliche Modell.

**Aktuell**: Wir belassen es bei **Option A (einfach)** â€“ K3s mit Zertifikat, oder warten auf die Traefik-Integration.

---

## 4. Traefik Ingress Controller (K3s Default)

K3s bringt Traefik standardmÃ¤ÃŸig mit.  
Traefik lÃ¤uft als **Deployment** im `kube-system`-Namespace:

```bash
kubectl get deploy -n kube-system traefik
```

### 4.1. Traefik Service und LoadBalancer

Traefik ist als **Service** vom Typ `LoadBalancer` konfiguriert:

```bash
kubectl get svc -n kube-system traefik
```

Ausgabe (Beispiel):

```text
NAME      TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
traefik   LoadBalancer   10.43.x.x       192.168.x.y   80:30080/TCP,443:30443/TCP   50m
```

**EXTERNAL-IP**: Dies ist die IP des Nodes, auf dem Traefik lÃ¤uft (oder eine Floating IP bei mehreren Nodes).

### 4.2. DaemonSet svclb-traefik

K3s nutzt sein eigenes **Service Load Balancer (svclb)**, um den Traefik-Service zu implementieren.  
Auf jedem Node lÃ¤uft ein `svclb-traefik-*`-Pod:

```bash
kubectl get pods -n kube-system | grep svclb-traefik
```

Diese Pods sorgen dafÃ¼r, dass eingehender Traffic auf Port 80/443 zu Traefik geleitet wird.

---

## 5. Ingress Routes fÃ¼r Services

### 5.1. Prinzip

Um einen Service im Cluster Ã¼ber `*.homehill.de` erreichbar zu machen:

1. **IngressRoute** (oder Ingress-Ressource) definieren
2. Host-Name angeben (z. B. `navidrome.homehill.de`)
3. TLS-Sertifikat (automatisch via Let's Encrypt oder manuell)
4. Traffic wird von Traefik zum Service weitergeleitet

Beispiel (zukÃ¼nftig):

```yaml
apiVersion: traefik.io/v1alpha1
kind: IngressRoute
metadata:
  name: navidrome
  namespace: default
spec:
  entryPoints:
    - web
    - websecure
  routes:
    - match: Host(`navidrome.homehill.de`)
      kind: Rule
      services:
        - name: navidrome
          port: 6595
  tls:
    certResolver: letsencrypt
    domains:
      - main: homehill.de
        sans:
          - '*.homehill.de'
```

### 5.2. TLS / Let's Encrypt Integration (TODO)

Aktuell ist dies noch **nicht konfiguriert**.  
Geplant ist die Integration von:

- **Let's Encrypt** (kostenlose Zertifikate)
- **Hetzner DNS API** (fÃ¼r DNS-01 Challenge)
- **Traefik CertResolver** (automatische Erneuerung)

Dies wird in einer separaten Dokumentation beschrieben: `docs/GITOPS-SETUP.md` oder `k8s/orchard/setup/traefik-letsencrypt.md` (TODO)

---

## 6. Netzwerk-Isolation und Segmentierung

### 6.1. Kubernetes-internes Netzwerk

K3s nutzt standardmÃ¤ÃŸig:

- **Service CIDR**: `10.43.0.0/16` (interne Cluster-Services)
- **Pod CIDR**: `10.42.0.0/16` (Pod-IPs)

Diese sind Ã¼ber das Kubernetes-Netzwerk-Plugin (Flannel in K3s) verbunden.

### 6.2. Namespace-Segregation (spÃ¤ter)

Langfristig wird Homehill-Struktur Ã¼ber Namespaces abgebildet:

- `default` â€“ fÃ¼r Test/Development
- `homelab-apps` â€“ produktive Services
- `monitoring` â€“ Prometheus, Grafana, etc.
- `system` â€“ Cluster-System-Komponenten (bereits `kube-system`, `kube-public`)

Pro Namespace spÃ¤ter auch RBAC-Policies.

---

## 7. DNS-AuflÃ¶sung im Cluster

### 7.1. CoreDNS

Der Cluster-DNS-Service lÃ¤uft als **Deployment** im `kube-system`-Namespace:

```bash
kubectl get deploy -n kube-system coredns
```

CoreDNS lÃ¶st auf:

- **Intra-Cluster-Services**: `service-name.namespace.svc.cluster.local`
- **Externe Domains**: (forward an externe Resolver, z. B. 1.1.1.1)

### 7.2. Service Discovery

Ein Pod kann einen anderen Service erreichen Ã¼ber:

```
http://service-name.namespace.svc.cluster.local:port
```

Beispiel:

```bash
# Innerhalb des Clusters
curl http://navidrome.default.svc.cluster.local:6595
```

---

## 8. Netzwerk-Policies (zukÃ¼nftig)

Aktuell sind **keine NetworkPolicies** definiert.  
Das bedeutet: Jeder Pod kann mit jedem anderen Pod kommunizieren.

ZukÃ¼nftig kÃ¶nnen NetworkPolicies eingefÃ¼hrt werden, um:

- Services voneinander zu isolieren
- Nur notwendige Kommunikation zu erlauben
- Sicherheit im Cluster zu erhÃ¶hen

Beispiel-Pattern:

- `monitoring`-Namespace darf Metriken von allen scrapen
- `homelab-apps` untereinander darf kommunizieren
- Externe Kommunikation nur auf Port 80/443 (HTTP/HTTPS)

---

## 9. Firewall und externe Zugriffe

### 9.1. Heimnetzwerk-Firewall

Der Cluster ist standardmÃ¤ÃŸig **nur im lokalen Heimnetzwerk** erreichbar.

- K3s API-Server: nur von `bearcube` (Desktop) erreichbar
- Traefik Ingress (Port 80/443): nur von lokalen GerÃ¤ten erreichbar

### 9.2. Externe Zugriffe (nicht empfohlen)

Falls Services spÃ¤ter Ã¼ber das Internet erreichbar sein sollen:

- Port-Forwarding im Home-Router einrichten (z. B. Port 443 â†’ `192.168.x.y:443`)
- Dann: FQDN muss extern aufgelÃ¶st werden (z. B. DynDNS mit Hetzner)
- TLS ist dann **kritisch** (Let's Encrypt mit DNS-01)

**Empfehlung fÃ¼r Homelab**: Lokal bleiben, externe Zugriffe Ã¼ber VPN.

---

## 10. Monitoring und Troubleshooting

### 10.1. Netzwerk-Debugging im Cluster

```bash
# Pods sehen und ihre IPs
kubectl get pods -A -o wide

# Service-IPs
kubectl get svc -A

# Endpoints (wo lÃ¤uft eigentlich ein Service?)
kubectl get endpoints -A

# Netzwerk-Policies (wenn vorhanden)
kubectl get networkpolicies -A

# DNS-Test (im Container)
kubectl run -it --rm debug --image=alpine --restart=Never -- sh
# Im Container:
nslookup kubernetes.default
nslookup google.com
```

### 10.2. HÃ¤ufige Netzwerk-Probleme

| Problem | Ursache | LÃ¶sung |
|---------|--------|--------|
| Pod kann Service nicht erreichen | DNS nicht aufgelÃ¶st | `kubectl logs -n kube-system deploy/coredns` |
| Ingress-Route funktioniert nicht | Traefik nicht ready | `kubectl get pods -n kube-system traefik-*` |
| Externe Domain nicht erreichbar | kein TLS-Zertifikat | Let's Encrypt + Traefik Setup (TODO) |
| IP-Adresse Ã¤ndert sich nach Reboot | DHCP-Reservation nicht aktiv | DHCP-Reservation fÃ¼r Node-MACs einrichten |

---

## 11. Zusammenfassung der Netzwerk-Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Homehill LAN (192.168.x.0/24)     â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Orchard K3s Cluster            â”‚  â”‚
â”‚  â”‚                                 â”‚  â”‚
â”‚  â”‚  apple (Control-Plane)          â”‚  â”‚
â”‚  â”‚  lemon (Worker)                 â”‚  â”‚
â”‚  â”‚  plum (Worker)                  â”‚  â”‚
â”‚  â”‚                                 â”‚  â”‚
â”‚  â”‚  Traefik (Ingress Controller)   â”‚  â”‚
â”‚  â”‚  CoreDNS (Cluster DNS)          â”‚  â”‚
â”‚  â”‚  Local-Path-Provisioner (Storage) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â”‚  Pi-hole DNS (lokale AuflÃ¶sung)       â”‚
â”‚                                         â”‚
â”‚  Hetzner DNS (external, homehill.de)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 12. NÃ¤chste Schritte

1. **DHCP-Reservations einrichten** fÃ¼r `apple`, `lemon`, `plum`
2. **DNS-EintrÃ¤ge** fÃ¼r `*.homehill.de` bei Hetzner konfigurieren
3. **Traefik + Let's Encrypt** konfigurieren (separate Doku)
4. **Erste IngressRoute** deployen (z. B. fÃ¼r Dashboard oder Test-Service)
5. **NetworkPolicies** spÃ¤ter, wenn Production-Services laufen

---

*Orchard wÃ¤chst â€“ von physischer Hardware Ã¼ber Netzwerk bis zur virtuellen Service-Welt.* ğŸŒ³ğŸ’š
