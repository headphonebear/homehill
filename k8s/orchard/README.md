# ğŸŒ³ Orchard â€“ Homehill Kubernetes Cluster

Orchard ist der Kubernetesâ€‘Cluster im Homehillâ€‘Homelab.  
Er ersetzt nach und nach Teile der bisherigen Dockerâ€‘Swarmâ€‘Umgebung durch eine K3sâ€‘basierte Orchestrierung, bleibt aber eng in die bestehende Homehillâ€‘Architektur integriert.

Ziel von Orchard:

- Plattform fÃ¼r zentrale Homelabâ€‘Dienste (Nextcloud, Immich, Keycloak, Monitoring, Slackâ€‘Bridge, AIâ€‘Tools usw.)
- GitOpsâ€‘freundliche Basis (Argo CD), um Konfigurationen reproduzierbar und versionskontrolliert zu verwalten
- Saubere Trennung zwischen â€altenâ€œ Swarmâ€‘Stacks und â€neuenâ€œ Kubernetesâ€‘Workloads

---

## ğŸ§± Cluster Overview

**Distribution:** K3s (Lightweight Kubernetes)  
**Version:** v1.33.6+k3s1  
**Control Plane:** Singleâ€‘node control plane auf `apple`  
**Worker Nodes:** `lemon`, `plum`  
**Cluster Name (kubectl context):** `orchard`

### Nodes

Die drei Orchardâ€‘Nodes laufen auf identischer NucBoxâ€‘G3â€‘Hardware (Alpine Linux):

- `apple` â€“ Controlâ€‘Plane / Master
- `lemon` â€“ Worker
- `plum` â€“ Worker

(Die ursprÃ¼nglichen Swarmâ€‘Nodes `nook`, `greenhouse`, `dovecote` und weitere GerÃ¤te wie NAS und Desktop sind im Homehillâ€‘Inventar dokumentiert und bleiben als Infrastrukturâ€‘Backbone bestehen.)

---

## ğŸ§© Packaged Components (K3s Defaults)

Direkt nach der K3sâ€‘Installation laufen im Cluster nur die von K3s mitgelieferten Komponenten:

- **CoreDNS** â€“ DNSâ€‘Service fÃ¼r den Cluster
- **Traefik** â€“ Ingress Controller & Reverse Proxy (K3sâ€‘Deployment)
- **local-path-provisioner** â€“ StorageClass fÃ¼r einfache lokale PersistentVolumes
- **metricsâ€‘server** â€“ Ressourcenmetriken fÃ¼r `kubectl top` & HPA
- **svclbâ€‘Pods fÃ¼r Traefik** â€“ LoadBalancerâ€‘Implementierung per DaemonSet (ein Pod pro Node)

Kontrolle direkt nach dem Bootstrap:

```bash
kubectl get nodes
kubectl get pods -A
```

---

## ğŸŒ Netzwerk & Zugriff

- **APIâ€‘Server:**  
  Intern aktuell per IP erreichbar (z.B. `https://192.168.x.y:6443`).  
  Geplant: FQDN wie `apple.homehill.de` bzw. `orchard-api.homehill.de` mit gÃ¼ltigem TLSâ€‘Zertifikat.
- **kubectl Zugriff:**  
  Auf dem Desktop (`bearcube`) ist `kubectl` installiert; die `kubeconfig` wurde von `apple` kopiert und auf die interne IP des APIâ€‘Servers angepasst.  
  Der aktive Kontext heiÃŸt:

  ```bash
  kubectl config current-context
  # orchard
  ```

- **DNS / Externe Domains:**  
  Die bestehende Homehillâ€‘DNSâ€‘Infrastruktur (`*.homehill.de` via Piâ€‘hole und Hetznerâ€‘DNS) wird spÃ¤ter fÃ¼r Ingressâ€‘Routen genutzt.  
  Ziel: Traefik + Letâ€™s Encrypt (DNSâ€‘01 Challenge via Hetzner) fÃ¼r Wildcardâ€‘Zertifikate.

---

## ğŸ‘¥ Identity & Security (Homehillâ€‘Schema)

Orchard orientiert sich am bestehenden Homehillâ€‘UID/GIDâ€‘ und Rollenâ€‘Schema (Systemâ€‘User, Bearâ€‘IdentitÃ¤ten, Serviceâ€‘Goblins, AIâ€‘Assistenten).  
Langfristig wird dieses Modell auf Kubernetes Ã¼bertragen, u.â€¯a. durch:

- Namespaces entsprechend Rollen / Projekten
- ServiceAccounts und RBAC, die â€Serviceâ€‘Goblinsâ€œ abbilden
- Trennung zwischen **Content Owner** (z.B. `mk3`, `coder`) und **Service Runner** (technische Accounts / ServiceAccounts)
- eigene IdentitÃ¤ten fÃ¼r AIâ€‘Assistenten (z.B. Ana) im Zugriff auf Clusterâ€‘APIs

Aktuell ist der Cluster im **Bootstrapâ€‘Zustand** ohne fein granulare RBACâ€‘Konfiguration; das folgt mit den ersten produktiven Workloads.

---

## ğŸ“Œ Aktueller Status

- K3s v1.33.6 ist auf allen drei Nodes installiert.
- `apple` fungiert als Controlâ€‘Plane, `lemon` und `plum` sind erfolgreiche Workerâ€‘Nodes.
- `kubectl` ist auf dem Desktop eingerichtet, Kontext `orchard` aktiv.
- Alle K3sâ€‘Coreâ€‘Komponenten laufen (`coredns`, `traefik`, `local-path-provisioner`, `metrics-server`, `svclb-traefik`).
- Noch keine produktiven Workloads; Orchard ist bereit fÃ¼r die ersten â€richtigenâ€œ Deployments.

---

## ğŸš§ NÃ¤chste Schritte

1. **TLS & Ingress**
   - Traefik konfigurieren fÃ¼r:
     - Letâ€™sâ€‘Encryptâ€‘Zertifikate (DNSâ€‘01 Challenge via Hetzner DNS API)
     - saubere Hosts wie `*.homehill.de` fÃ¼r Kubernetesâ€‘Services
   - ggf. spÃ¤ter Umstieg auf certâ€‘manager, wenn erforderlich.

2. **GitOps / Argo CD**
   - `k8s/orchard/gitops/` aufsetzen:
     - `base/` fÃ¼r Namespaces, grundlegende Infrastruktur (Monitoring, Ingress, Storage, Logging)
     - `argo-apps/` fÃ¼r Appâ€‘ofâ€‘Appsâ€‘Pattern
   - Argo CD im Cluster deployen und mit diesem Repository verdrahten.

3. **Workloadâ€‘Migration & neue Services**
   - Geplante Projekte (Nextcloud, Immich, Jellyfinâ€‘Nachfolger, Keycloak, Uptime Kuma, etc.) schrittweise auf Orchard bringen.
   - Services aus der Swarmâ€‘Welt nach und nach migrieren, **nur** wenn sie sowieso angefasst werden (â€Never stop a running systemâ€œ).

4. **Monitoring & Runbooks**
   - Kubernetesâ€‘Monitoring (z.B. Netdata/Grafana/Prometheus) fÃ¼r Orchard etablieren.
   - Runbooks fÃ¼r hÃ¤ufige Tasks und Incidentâ€‘Response ergÃ¤nzen.

---

## ğŸ“‚ Weitere Dokumentation

- `k8s/orchard/setup/INSTALLATION.md`  
  Detaillierte Schrittâ€‘fÃ¼râ€‘Schrittâ€‘Anleitung: Von der NucBoxâ€‘G3â€‘Installation (Alpine Linux) bis zum ersten erfolgreichen `kubectl get pods -A`.

- `docs/ARCHITECTURE.md`  
  GesamtÃ¼bersicht Ã¼ber Homehill: Swarmâ€‘Cluster, Orchardâ€‘Cluster, NAS, Piâ€‘hole, Netzwerkâ€‘Topologie.

- `docs/GITOPS-SETUP.md`  
  Design und Implementierung der GitOpsâ€‘Pipeline (Argo CD, Repos, Branchingâ€‘Strategie).

---

*Orchard ist das Kubernetesâ€‘Herz von Homehill â€“ gewachsen aus spÃ¤ten NÃ¤chten, Tripâ€‘Hopâ€‘Beats und einer Menge Liebe zum Detail.* ğŸŒ³ğŸ’š
