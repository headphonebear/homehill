# Orchard â€“ Installation und Bootstrap

Dieses Dokument beschreibt die Installation des Orchardâ€‘Clusters, vom ersten Setup der drei NucBoxâ€‘G3â€‘Maschinen mit Alpine Linux bis zum ersten erfolgreichen `kubectl get pods -A` vom Desktop aus.

Orchard besteht aus einem einzelnen K3sâ€‘Controlâ€‘Planeâ€‘Node (`apple`) und zwei Workerâ€‘Nodes (`lemon`, `plum`).  

Alle Schritte sind so dokumentiert, dass ein Rebuild (z. B. nach Hardwaretausch oder Neuinstallation) reproduzierbar mÃ¶glich ist.

---

## 1. Ziel und Ãœberblick

Ziel dieser Installation:

- Aufbau eines kleinen, aber vollwertigen Kubernetesâ€‘Clusters auf Basis von K3s.
- Nutzung von drei identischen NucBox G3 als physische Nodes.
- Zugriff auf den Cluster von einem separaten Desktopâ€‘Rechner (`bearcube`) per `kubectl`.
- Vorbereitung fÃ¼r spÃ¤teres GitOps (Argo CD) und Ingress/TLSâ€‘Konfiguration (Traefik + Let's Encrypt).

Kernentscheidungen:

- **K3s** statt â€žFull" Kubernetes (kubeadm), um Ressourcen auf den NUCs zu schonen.
- **Alpine Linux** als minimalistisches, schnelles Basisâ€‘OS.
- Ein **einzelner Controlâ€‘Planeâ€‘Node** (fÃ¼r Homelab vÃ¶llig ausreichend).
- K3sâ€‘Defaultâ€‘Komponenten (Traefik, CoreDNS, local-path-provisioner, metricsâ€‘server) werden genutzt und nicht deaktiviert.

---

## 2. Hardware und Rollen

Alle drei Orchardâ€‘Nodes basieren auf gleicher Hardware:

- Modell: NucBox G3
- Rolle im Cluster:
  - `apple` â€“ Controlâ€‘Plane / Master
  - `lemon` â€“ Worker
  - `plum` â€“ Worker

Die konkrete Hardwareâ€‘Spezifikation (CPU, RAM, SSDâ€‘GrÃ¶ÃŸe, Besonderheiten) wird in einem separaten Dokument festgehalten:

- `k8s/orchard/setup/hardware-specs.md` (TODO)

---

## 3. Alpine Linux auf den NucBox G3 installieren

Jeder der drei Nodes erhÃ¤lt eine frische Alpineâ€‘Installation. Die Schritte sind im Prinzip identisch, Unterschiede gibt es nur bei Hostname und IP/Netzwerk.

### 3.1. Boot von Alpineâ€‘Installationsmedium

1. Alpine Linux (Extended) ISO herunterladen.
2. BootfÃ¤higen USBâ€‘Stick erstellen.
3. NucBox G3 von USB booten.
4. Im Alpineâ€‘Bootâ€‘MenÃ¼ die Standardâ€‘Option wÃ¤hlen (Extended Variante).

### 3.2. Basisinstallation

Auf jedem Node:

- Als `root` anmelden (kein Passwort).
- Den interaktiven Installer starten:

  ```sh
  setup-alpine
  ```

- Wichtige Entscheidungen wÃ¤hrend `setup-alpine`:
  - Keyboard layout, Zeitzone, Locale â†’ passend zu Homehill.
  - Hostname:  
    - `apple`  
    - `lemon`  
    - `plum`
  - Netzwerk:
    - Entweder statische IPs im Homehillâ€‘Netz
    - oder DHCP, solange die IPs zuverlÃ¤ssig sind bzw. via DHCPâ€‘Reservation festgezurrt werden.
  - SSHâ€‘Server: `openssh` installieren und aktivieren.
  - Diskâ€‘Layout:
    - Ãœbliche Partitionierung (root + ggf. separate EFI/Boot).
    - Alpine auf die interne SSD installieren.

Nach Abschluss:

- USBâ€‘Stick entfernen.
- Neu starten.
- Per SSH von einem anderen Host einloggen (oder direkt an der Konsole weiterarbeiten).

---

## 4. Grundkonfiguration auf allen Nodes

Auf jedem der drei Nodes:

### 4.1. Aktualisieren und Basispakete installieren

```sh
apk update
apk upgrade
apk add htop curl nano
```

Optional: Zeitsynchronisation und kleine Komfortâ€‘Settings (z. B. `chrony`, Alias fÃ¼r `kubectl`, etc.). FÃ¼r den Bootstrap reicht das Minimum.

### 4.2. ÃœberprÃ¼fen von Netzwerk und DNS

Sicherstellen, dass:

- Internetzugang vorhanden ist (`ping 1.1.1.1`, `ping github.com`).
- NamensauflÃ¶sung fÃ¼r `*.homehill.de` funktioniert (falls bereits eingerichtet).
- Die Nodes sich gegenseitig pingen kÃ¶nnen (`ping apple`, `ping lemon`, `ping plum`), sofern DNS-EintrÃ¤ge existieren.

---

## 5. K3sâ€‘Server auf `apple` installieren

`apple` ist der Controlâ€‘Planeâ€‘Node (K3sâ€‘Server).

### 5.1. K3s Ã¼ber das offizielle Installâ€‘Script

Auf `apple` als `root`:

```sh
curl -sfL https://get.k3s.io | sh -
```

Wichtige Punkte:

- Das Script:
  - findet die aktuelle â€žstable" K3sâ€‘Version,
  - lÃ¤dt das passende Binary herunter,
  - verifiziert den Hash,
  - installiert `k3s` unter `/usr/local/bin`,
  - richtet den `k3s`â€‘Service ein und startet ihn.

Hinweis:  
Bei sehr langsamen Downloads von GitHub kann das Herunterladen des Binaries einige Zeit dauern, auch wenn die eigene Internetleitung schnell ist.

### 5.2. Erste Kontrolle des K3sâ€‘Servers

Nach erfolgreicher Installation sollte:

```sh
k3s kubectl get nodes
```

die Node `apple` als `Ready` mit Rolle `control-plane,master` anzeigen, z. B.:

```text
NAME    STATUS   ROLES                  AGE   VERSION
apple   Ready    control-plane,master   2m    v1.33.6+k3s1
```

### 5.3. Nodeâ€‘Token fÃ¼r Workerâ€‘Nodes auslesen

Damit `lemon` und `plum` dem Cluster als Worker beitreten kÃ¶nnen, wird der `node-token` vom Server benÃ¶tigt:

```sh
cat /var/lib/rancher/k3s/server/node-token
```

Den ausgegebenen Token sicher kopieren â€“ er wird auf den Workerâ€‘Nodes als `K3S_TOKEN` verwendet.

---

## 6. K3sâ€‘Agent auf `lemon` und `plum` installieren

Auf `lemon` und `plum` wird K3s als Agent (Workerâ€‘Node) installiert. Beide verbinden sich mit dem APIâ€‘Server auf `apple`.

### 6.1. K3sâ€‘Agent mit K3S_URL und K3S_TOKEN

Auf `lemon` als `root`:

```sh
curl -sfL https://get.k3s.io | \
  K3S_URL="https://192.168.x.y:6443" \
  K3S_TOKEN="HIER_DEN_TOKEN_VON_APPLE_EINFÃœGEN" \
  sh -
```

Auf `plum` analog:

```sh
curl -sfL https://get.k3s.io | \
  K3S_URL="https://192.168.x.y:6443" \
  K3S_TOKEN="HIER_DEN_TOKEN_VON_APPLE_EINFÃœGEN" \
  sh -
```

Hinweis:  
WÃ¤hrend des ersten Bootstraps wird die **IPâ€‘Adresse** des APIâ€‘Servers verwendet (`https://192.168.x.y:6443`), da fÃ¼r `apple.homehill.de` noch kein gÃ¼ltiges TLSâ€‘Zertifikat konfiguriert ist.  

Beides ist prinzipiell mÃ¶glich; langfristig ist der FQDN mit passendem Zertifikat wÃ¼nschenswert.

### 6.2. Kontrolle vom Server aus

ZurÃ¼ck auf `apple` (oder vom Desktop mit `kubectl`):

```sh
k3s kubectl get nodes
```

Erwartete Ausgabe:

```text
NAME    STATUS   ROLES                  AGE   VERSION
apple   Ready    control-plane,master   45m   v1.33.6+k3s1
lemon   Ready    <none>                 14m   v1.33.6+k3s1
plum    Ready    <none>                 15m   v1.33.6+k3s1
```

Damit ist der Cluster aus Sicht von K3s vollstÃ¤ndig: eine Controlâ€‘Plane, zwei Workerâ€‘Nodes.

---

## 7. kubectl auf dem Desktop einrichten

Der tÃ¤gliche Zugriff auf den Cluster erfolgt von einem Desktopâ€‘Rechner (z. B. `bearcube`), auf dem `kubectl` installiert wird.

### 7.1. kubectl installieren

Unter Linux (Ubuntuâ€‘Basis) empfiehlt sich die offizielle Kubernetesâ€‘Doku:

1. Nach Anleitung von  
   https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/  
   `kubectl` installieren.
2. Sicherstellen, dass `kubectl` im `PATH` liegt:

   ```sh
   kubectl version --client
   ```

### 7.2. kubeconfig von `apple` kopieren

Auf `apple` liegt die K3sâ€‘kubeconfig normalerweise unter:

```text
/etc/rancher/k3s/k3s.yaml
```

Diese Datei wird auf den Desktop kopiert, z. B.:

```sh
# auf apple
scp /etc/rancher/k3s/k3s.yaml user@bearcube:/home/user/.kube/config
```

(Ordner `~/.kube` auf `bearcube` ggf. vorher anlegen.)

### 7.3. Serverâ€‘Adresse anpassen

Die von K3s erzeugte `k3s.yaml` verwendet standardmÃ¤ÃŸig `https://127.0.0.1:6443` als APIâ€‘Endpoint.  

Auf dem Desktop muss diese Adresse angepasst werden auf die IP von `apple`:

```yaml
server: https://192.168.x.y:6443
```

(SpÃ¤ter, wenn TLS/Ingress stimmt, kann das auf einen FQDN umgestellt werden.)

### 7.4. Kontext umbenennen: `default` â†’ `orchard`

Die von K3s bereitgestellte kubeconfig verwendet oft Ã¼berall `default` als Namen fÃ¼r:

- Cluster
- Context
- User
- `current-context`

Zur besseren Lesbarkeit und Klarheit wird alles auf `orchard` umgestellt. Mit `sed`:

```sh
sed -i 's/name: default/name: orchard/g' ~/.kube/config
sed -i 's/cluster: default/cluster: orchard/g' ~/.kube/config
sed -i 's/user: default/user: orchard/g' ~/.kube/config
sed -i 's/current-context: default/current-context: orchard/g' ~/.kube/config
```

Danach prÃ¼fen:

```sh
kubectl config current-context
# erwartet: orchard
```

---

## 8. Erste Cluster-Checks

Mit eingerichtetem `kubectl` und aktivem `orchard`â€‘Kontext:

### 8.1. Nodes

```sh
kubectl get nodes
```

Erwartete Ausgabe (Beispiel):

```text
NAME    STATUS   ROLES                  AGE   VERSION
apple   Ready    control-plane,master   45m   v1.33.6+k3s1
lemon   Ready    <none>                 14m   v1.33.6+k3s1
plum    Ready    <none>                 15m   v1.33.6+k3s1
```

### 8.2. Systemâ€‘Pods

```sh
kubectl get pods -A
```

Typische Ausgabe direkt nach der Installation:

```text
NAMESPACE     NAME                                      READY   STATUS      RESTARTS   AGE
kube-system   coredns-6d668d687-8swbm                   1/1     Running     0          50m
kube-system   helm-install-traefik-65f9n                0/1     Completed   1          50m
kube-system   helm-install-traefik-crd-nh54t            0/1     Completed   0          50m
kube-system   local-path-provisioner-869c44bfbd-vjwbp   1/1     Running     0          50m
kube-system   metrics-server-7bfffcd44-6j7p8            1/1     Running     0          50m
kube-system   svclb-traefik-7a9db005-8k7hm              2/2     Running     0          50m
kube-system   svclb-traefik-7a9db005-jg7vx              2/2     Running     0          20m
kube-system   svclb-traefik-7a9db005-r4g4x              2/2     Running     0          20m
kube-system   traefik-865bd56545-xfp82                  1/1     Running     0          50m
```

Damit ist der Cluster funktionsfÃ¤hig und bereit fÃ¼r weitere Konfiguration (Ingress/TLS, Namespaces, GitOps usw.).

---

## 9. Bekannte EinschrÃ¤nkungen und offene Punkte

Zum Zeitpunkt dieser Installation sind folgende Punkte bewusst noch offen bzw. provisorisch:

1. **FQDN und TLS fÃ¼r den APIâ€‘Server**
   - Aktuell wird der APIâ€‘Server Ã¼ber eine interne IP angesprochen.
   - Geplant ist die Nutzung eines FQDN wie `apple.homehill.de` oder `orchard-api.homehill.de` mit gÃ¼ltigem TLSâ€‘Zertifikat.
   - Dazu wird entweder K3s selbst oder ein vorgelagerter Reverse Proxy (Traefik mit Let'sâ€‘Encryptâ€‘Zertifikat) genutzt.

2. **Ingress und Public Services**
   - Traefik ist als Ingress Controller bereits aktiv, aber noch nicht fÃ¼r produktive IngressRoutes konfiguriert.
   - Geplant:
     - DNSâ€‘01â€‘Challenge via Hetznerâ€‘DNS fÃ¼r Let'sâ€‘Encryptâ€‘Zertifikate.
     - Nutzung von `*.homehill.de` fÃ¼r die Dienste im Cluster.

3. **GitOps / Argo CD**
   - Der Orchardâ€‘Cluster ist noch nicht an Argo CD angebunden.
   - Das Repository (`homehill`) wird vorbereitet, um spÃ¤ter:
     - Namespaces, Basisâ€‘Infrastruktur und Workloads deklarativ zu definieren.
     - Ã„nderungen Ã¼ber Gitâ€‘Commits auszurollen.

4. **Security & RBAC**
   - Aktuell lÃ¤uft der Cluster noch ohne feingranulares RBACâ€‘Modell.
   - Ziel ist die Abbildung des bestehenden Homehillâ€‘IdentitÃ¤tsâ€‘Schemas:
     - Systemâ€‘Benutzer, Serviceâ€‘Goblins, AIâ€‘IdentitÃ¤ten
     - Namespaces, ServiceAccounts, Rollen und RoleBindings.

---

## 10. Zusammenfassung

Mit diesen Schritten ist Orchard als dreikÃ¶pfiger K3sâ€‘Cluster erfolgreich installiert:

- Drei Alpineâ€‘basierte NucBox G3â€‘Nodes (`apple`, `lemon`, `plum`).
- K3sâ€‘Server auf `apple`, Agents auf `lemon` und `plum`.
- Zugriff via `kubectl` vom Desktop, Kontext `orchard`.
- Alle K3sâ€‘Coreâ€‘Komponenten laufen stabil und bereit fÃ¼r weitere Konfiguration.

Die nÃ¤chsten Schritte (separate Dokumente):

- `k8s/orchard/README.md` â†’ Ãœberblick Ã¼ber den Cluster und seine Rolle in Homehill.
- `k8s/orchard/setup/network.md` â†’ Netzwerkâ€‘Details (IPs, DNS, Piâ€‘hole, Hetzner).
- `docs/GITOPS-SETUP.md` â†’ Design und Aufbau der GitOpsâ€‘Pipeline mit Argo CD.

Orchard ist damit das Kubernetesâ€‘Herz von Homehill â€“ bereit, mit Leben gefÃ¼llt zu werden. ðŸŒ³ðŸ’š
